{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dee9154",
   "metadata": {},
   "source": [
    "### KAYAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881ad61",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous verrons toute la partie **Scraping de données** pour récupérer les informations concernant les hotels qui se trouvent dans les villes que nous avons choisies d'étudier.\n",
    "\n",
    "Il y a plusieurs techniques de Data Mining : \n",
    "* soit on utilise la librairie `requests` pour le faire mais elle est assez limitée\n",
    "* soit on peut utiliser `Beautiful Soup` ou `Selenium` qui sont assez complet pour scraper les données \n",
    "* soit on peut utiliser `Scrapy` qui est assez complexe mais plus complet que ceux cités avant.\n",
    "\n",
    "Notre choix se portent donc sur `Scrapy`\n",
    "\n",
    "Nous allons utiliser l'url de recherche : https://www.booking.com/searchresults.fr.html pour faire nos requètes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9339039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea148e",
   "metadata": {},
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d540cd6f",
   "metadata": {},
   "source": [
    "/kayak\n",
    "   |__scrapy.cfg                 # fichier qui definit le projet\n",
    "   |__/kayak\n",
    "         |__items.py             # fichier qui definit les models items (type de données)\n",
    "         |__middlewares.py       # fichier qui definit comment agiront les middlewares\n",
    "         |__pipelines.py         # fichier qui permet de faire des pipelines de parsing\n",
    "         |__settings.py          # fichier qui definit tous les settings\n",
    "         |__ __init__.py          \n",
    "         |__/spider\n",
    "               |__ __init__.py   # fichier qui initialise le spider\n",
    "               |__CONSTANTES.py  # fichier qui regroupe les filtres\n",
    "               |__hb.py          # fichier du Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad624e",
   "metadata": {},
   "source": [
    "L'Objectif est de recupérer pleins d'informations sur les hotels grace à 3 methodes successives du Spider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef12ba",
   "metadata": {},
   "source": [
    "Notre spider va effectuer 3 étapes pour scraper Booking :\n",
    "\n",
    "* La 1ere etape consiste à entrer une url de recherche avec le nom de la ville et différents filtres de recherche dans un formulaire avec la methode de Spider : `scrapy.FormRequest.from_response()`\n",
    "\n",
    "* La 2nde etape va être d'utiliser le callback du FormRequest pour aller sur la page des 25 meilleurs hotels et d'en extraire les informations utiles (nom de l'hotel, sa description, son lien url, sa note / 5, ect...) car chaque hotel est représenté sur cette page par un encadré qui est dans une div `'//div[@data-testid=\"property-card\"]'` et qui regroupe toutes ces informations\n",
    "\n",
    "* La 3eme etape va permettre de recupérer les coordonnées de chaque hotel en faisant un `response.follow()` pour conserver les informations que l'on vient de parser grace au `meta` du follow() et on appelle la nouvelle methode grace au callback\n",
    "\n",
    "On effectue ce processus dans uen boucle qui itère sur tous les villes et recupère ainsi 25 hotels par ville."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb248ebf",
   "metadata": {},
   "source": [
    "Il existe une grande quantité de filtres pour affiner ses recherches : chaque critère de recherche est séparé par un `&` dans l'url.\n",
    "\n",
    "par exemple:\n",
    "\n",
    "* pour rechercher la ville de Paris, il suffit me mettre le filtre : `&ss='Paris'`\n",
    "* pour ne choisir que des hotels, il suffit de mettre le filtre : `&dest_type='hotel'`\n",
    "* pour choisir 3 chambres, on met le filtre : `&no_rooms=3`\n",
    "\n",
    "On va utiliser comme technique celle qui nous permet de reconstruire une url à partir de filtres pour faire la 1ere étape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3338a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.booking.com/searchresults.fr.html?aid=397594\n",
    "        &label=gog235jc-1DCAEoggI46AdIDVgDaE2IAQGYAQ24ARfIAQzYAQPoAQH4AQKIAgGoAgO4Asyp0ZkGwAIB0gIkZjA4ZGZiMGYtMGJmZC00MTEzLTgxOWYtMGYxNDk1ZWYzMTAx2AIE4AIB\n",
    "        &sid=9a0ba32de0342ed74b61838fa4ac7c7b\n",
    "        &sb=1\n",
    "        &sb_lp=1\n",
    "        &src=index\n",
    "        &src_elem=sb\n",
    "        &error_url=https%3A%2F%2Fwww.booking.com%2Findex.fr.html%3Faid%3D397594%26label%3Dgog235jc-1DCAEoggI46AdIDVgDaE2IAQGYAQ24ARfIAQzYAQPoAQH4AQKIAgGoAgO4Asyp0ZkGwAIB0gIkZjA4ZGZiMGYtMGJmZC00MTEzLTgxOWYtMGYxNDk1ZWYzMTAx2AIE4AIB%26sid%3D9a0ba32de0342ed74b61838fa4ac7c7b%26sb_price_type%3Dtotal%26%26\n",
    "        &ss=Paris\n",
    "        &is_ski_area=0\n",
    "        &checkin_year=\n",
    "        &checkin_month=\n",
    "        &checkout_year=\n",
    "        &checkout_month=\n",
    "        &group_adults=2\n",
    "        &group_children=0\n",
    "        &no_rooms=1\n",
    "        &b_h4u_keep_filters=\n",
    "        &from_sf=1\n",
    "        &dest_id=\n",
    "        &dest_type=\n",
    "        &search_pageview_id=d4b26326b5ea0299\n",
    "        &search_selected=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_url:\n",
    "    https://www.booking.com/searchresults.fr.html?aid=397594&ss=Paris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214204e",
   "metadata": {},
   "source": [
    "Ceci est le fichier `CONSTANTES.py` qui est dans le meme repertoire que le script du Spider et qui permet de faire plusieurs actions :\n",
    "\n",
    "* Definir des filtres pour avoir une recherche plus ciblée\n",
    "* Convertir des caractères spéciaux en caractères normaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fichier CONSTANTES.py\n",
    "\n",
    "NB_ETOILE = 4                  # 4 etoiles\n",
    "BUDGET_100_150 = 3             # budget entre 100 et 150\n",
    "BUDGET_150_200 = 4             # budget entre 150 et 200\n",
    "SCORE_8_SUR_10 = 80            # note minimum de 8/10 \n",
    "DISTANCE_CENTRE_VILLE = 3000   # distance moins de 3km du centre ville\n",
    "\n",
    "\n",
    "cities = [\"Mont Saint Michel\",\"St Malo\",\"Bayeux\",\"Le Havre\",\"Rouen\",\"Paris\",\"Amiens\",\"Lille\",\"Strasbourg\",\n",
    "          \"Chateau du Haut Koenigsbourg\",\"Colmar\",\"Eguisheim\",\"Besancon\",\"Dijon\",\"Annecy\",\"Grenoble\",\"Lyon\",\n",
    "          \"Gorges du Verdon\",\"Bormes les Mimosas\",\"Cassis\",\"Marseille\",\"Aix en Provence\",\"Avignon\",\"Uzes\",\n",
    "          \"Nimes\",\"Aigues Mortes\",\"Saintes Maries de la mer\",\"Collioure\",\"Carcassonne\",\"Ariege\",\"Toulouse\",\n",
    "          \"Montauban\",\"Biarritz\",\"Bayonne\",\"La Rochelle\"]\n",
    "\n",
    "d = {\"\\u00e8\" : \"e\",\n",
    " \"\\u00e9\" : \"e\",\n",
    " \"\\u00ee\" : \"i\",\n",
    " \"\\u00d4\" : \"O\",\n",
    " \"\\u00c2\" : \"A\",\n",
    " \"\\u00b0\" : \"\",\n",
    " \"\\u0153\" : \"oe\",\n",
    " \"\\u00e7\" : \"c\",\n",
    " \"\\u00f2\" : \"o\",\n",
    " \"\\u00ea\" : \"e\",\n",
    " \"\\u00e0\" : \"a\",\n",
    " \"\\u00e9\" : \"e\",\n",
    " \"\\u2019\" : \"'\",\n",
    " \"\\u00e2\" : \"a\",\n",
    " \"\\u00c9\" : \"E\",\n",
    " \"\\u2606\" : \" \",\n",
    " \"\\u2729\" : \" \",\n",
    " \"\\u00f4\" : \"o\",\n",
    " \"\\u00e8\" : \"e\",\n",
    " \"\\u00ee\" : \"i\",\n",
    " \"\\u00fb\" : \"u\",\n",
    " \"\\u00e9\" : \"e\",\n",
    " \"\\u00e7\" : \"c\",\n",
    " \"\\u00e2\" : \"a\",\n",
    " \"\\u2122\" : \"\"}\n",
    "\n",
    "def utf8(dic, string):\n",
    "    x = \"\"\n",
    "    for el in string:\n",
    "        if el in dic:\n",
    "            x += dic[el]\n",
    "        else:\n",
    "            x += el\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab094a2",
   "metadata": {},
   "source": [
    "Nous ouvrons le fichier texte contenant une liste de proxy pour eviter de se faire detecter en tant que bot de parsing et nous l'implementerons dans le fichier settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8762c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = r'C:\\Users\\david\\Downloads'\n",
    "file = os.path.join(p, '', 'proxy.txt')\n",
    "data = []\n",
    "with open(file, 'r') as f:\n",
    "    contents = f.read()\n",
    "    data.append(contents.replace(\"\\n\", ','))\n",
    "\n",
    "# On obtient toutes les adresses IP pour faire de la rotation de proxy\n",
    "txt_file = ''.join(data).split(',')[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa071d6",
   "metadata": {},
   "source": [
    "Nous mettons un delais entre 2 recupérations de données, nous choisissons un user_agent qui n'est pas celui par default (scrapy) pour ne pas se faire repérer comme bot, ect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### setting.py\n",
    "\n",
    "BOT_NAME = 'kayak'\n",
    "\n",
    "SPIDER_MODULES = ['kayak.spiders']\n",
    "NEWSPIDER_MODULE = 'kayak.spiders'\n",
    "\n",
    "\n",
    "\n",
    "USER_AGENT =  'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82 Safari/537.36'\n",
    "\n",
    "\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "\n",
    "DOWNLOAD_DELAY = 3\n",
    "\n",
    "COOKIES_ENABLED = False\n",
    "\n",
    "\n",
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n",
    "    'scrapy_user_agents.middlewares.RandomUserAgentMiddleware': 400,\n",
    "    'kayak.middlewares.KayakDownloaderMiddleware': 543,\n",
    "}\n",
    "\n",
    "\n",
    "AUTOTHROTTLE_ENABLED = True\n",
    "\n",
    "AUTOTHROTTLE_START_DELAY = 5\n",
    "\n",
    "AUTOTHROTTLE_MAX_DELAY = 60\n",
    "\n",
    "# proxy.txt\n",
    "ROTATING_PROXY_LIST = txt_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbc2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import logging\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class HotelsbookingSpider(scrapy.Spider):\n",
    "    name = 'hb'\n",
    "    allowed_domains = ['www.booking.com']\n",
    "    start_urls = ['https://www.booking.com/searchresults.fr.html?ss=']\n",
    "    \n",
    "    def parse(self, response):\n",
    "\n",
    "        cities = [\"Mont Saint Michel\",\"St Malo\",\"Bayeux\",\"Le Havre\",\"Rouen\",\"Paris\",\"Amiens\",\"Lille\",\"Strasbourg\",\n",
    "              \"Chateau du Haut Koenigsbourg\",\"Colmar\",\"Eguisheim\",\"Besancon\",\"Dijon\",\"Annecy\",\"Grenoble\",\"Lyon\",\n",
    "              \"Gorges du Verdon\",\"Bormes les Mimosas\",\"Cassis\",\"Marseille\",\"Aix en Provence\",\"Avignon\",\"Uzes\",\n",
    "              \"Nimes\",\"Aigues Mortes\",\"Saintes Maries de la mer\",\"Collioure\",\"Carcassonne\",\"Ariege\",\"Toulouse\",\n",
    "              \"Montauban\",\"Biarritz\",\"Bayonne\",\"La Rochelle\"]\n",
    "\n",
    "            result = (city for city in cities)\n",
    "            #filters = [f\"&nflt=class%3D{cst.NB_ETOILE}\", f\"%3Bpri%3D{cst.BUDGET_100_150}\", f\"%3Bpri%3D{cst.BUDGET_150_200}\", \\\n",
    "              #f\"%3Breview_score%3D{cst.SCORE_8_SUR_10}\", f\"%3Bdistance%3D{cst.DISTANCE_CENTRE_VILLE}\"]\n",
    "\n",
    "            for i in range(len(cities)):\n",
    "                yield scrapy.FormRequest.from_response(\n",
    "                    response,\n",
    "                    formdata={'ss': next(result)}, # + ''.join([f for f in filters])},\n",
    "                    callback=self.search\n",
    "                )\n",
    "\n",
    "    def search(self, response):\n",
    "        \n",
    "        hotels = response.xpath('//div[@data-testid=\"property-card\"]')\n",
    "\n",
    "        for hotel in hotels:\n",
    "            link = hotel.xpath('.//h3/a[@data-testid=\"title-link\"]/@href').get()\n",
    "            title =  hotel.xpath('.//h3/a[@data-testid=\"title-link\"]/div/text()').get(\"\").replace(\"\\u00e8\", \"e\").replace(\"\\u00e9\", \"e\").replace(\"\\u00ee\", \"i\").replace(\"\\u00d4\", \"O\").replace(\"\\u00c2\", \"A\").replace(\"\\u00b0\", \"\").replace(\"\\u0153\", \"oe\").replace(\"\\u00e7\", \"c\").replace(\"\\u00f2\", \"o\").replace(\"\\u00ea\", \"e\").replace(\"\\u00e0\", \"a\").replace(\"\\u00e9\", \"e\").replace(\"\\u2019\", \"'\").replace(\"\\u00e2\", \"a\").replace(\"\\u00c9\", \"E\").replace(\"\\u2606\", \" \").replace(\"\\u2729\", \" \").replace('\\u00f4', 'o').replace('\\u00e8','e').replace(\"\\u00ee\", \"i\").replace(\"\\u00fb\", \"u\").replace(\"\\u00e9\", \"e\").replace(\"\\u00e7\", \"c\").replace(\"\\u00e2\", \"a\").replace(\"\\u2122\", \"\")\n",
    "            #'title' : cst.utf8(cst.d, hotel.xpath('.//h3/a[@data-testid=\"title-link\"]/div/text()').get(\"\"))\n",
    "            price =  hotel.xpath('.//div[@data-testid=\"price-and-discounted-price\"]/text()').get()\n",
    "            location =  hotel.xpath('.//span[@data-testid=\"address\"]/text()').get(\"\")\n",
    "            score = hotel.xpath('.//div[@data-testid=\"review-score\"]/div/text()').get(\"\")\n",
    "            review_count = hotel.xpath('.//div[@data-testid=\"review-score\"]/div[2]/div[2]/text()').get(\"\")\n",
    "            stars = hotel.xpath('.//div[@data-testid=\"rating-stars\"]/span').getall()\n",
    "            description = hotel.xpath('//div[@class=\"d8eab2cf7f\"]/text()').get()\n",
    "            image = hotel.xpath('.//img[@data-testid=\"image\"]/@src').get()\n",
    "                                                                                                                                                                                                                                                                    \n",
    "            yield response.follow(url=link, callback=self.parse_url, meta={'link' : link, 'title' : title, 'price' : price, 'location' : location, 'score' : score, 'review_count' : review_count, 'stars' : stars, 'description': description, 'image': image})      \n",
    "            \n",
    "\n",
    "    def parse_url(self, response):\n",
    "\n",
    "        link = response.request.meta['link']\n",
    "        title = response.request.meta['title']\n",
    "        price = response.request.meta['price']\n",
    "        location = response.request.meta['location']\n",
    "        score = response.request.meta['score']\n",
    "        review_count = response.request.meta['review_count']\n",
    "        stars = response.request.meta['stars']\n",
    "        description = response.request.meta['description']\n",
    "        image = response.request.meta['image']\n",
    "\n",
    "        coords = response.xpath(\"//a[@data-atlas-latlng]/@data-atlas-latlng\").get(default='')\n",
    "\n",
    "        yield {\n",
    "               'link' : link,\n",
    "               'title' : title,\n",
    "               'price' :  price,\n",
    "               'location' : location,\n",
    "               'score' : score,\n",
    "               'review_count' : review_count,\n",
    "               'stars' : stars,\n",
    "               'description' : description,\n",
    "               'image' : image,\n",
    "               'coords' : coords,\n",
    "               'lat' : coords.split(\",\")[0],\n",
    "               'lon' : coords.split(\",\")[1]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901081c",
   "metadata": {},
   "source": [
    "le CrawlerProcess permet de scraper les données directement depuis un notebook : nous stockons les données dans un fichier `cities.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6533fb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-29 16:24:44 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-09-29 16:24:44 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.2.0, Python 3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1q  5 Jul 2022), cryptography 3.4.8, Platform Windows-10-10.0.19044-SP0\n",
      "2022-09-29 16:24:44 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:92.0) '\n",
      "               'Gecko/20100101 Firefox/92.0'}\n",
      "2022-09-29 16:24:44 [scrapy.extensions.telnet] INFO: Telnet Password: b62cb375b2cfc7bd\n",
      "2022-09-29 16:24:44 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.throttle.AutoThrottle']\n",
      "2022-09-29 16:24:44 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-09-29 16:24:44 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-09-29 16:24:44 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-09-29 16:24:44 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-09-29 16:24:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-09-29 16:24:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-09-29 16:25:03 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-09-29 16:25:03 [scrapy.extensions.feedexport] INFO: Stored json feed (825 items) in: cities.json\n",
      "2022-09-29 16:25:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 27541,\n",
      " 'downloader/request_count': 34,\n",
      " 'downloader/request_method_count/GET': 34,\n",
      " 'downloader/response_bytes': 5598532,\n",
      " 'downloader/response_count': 34,\n",
      " 'downloader/response_status_count/200': 34,\n",
      " 'elapsed_time_seconds': 19.013674,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 9, 29, 14, 25, 3, 330038),\n",
      " 'httpcompression/response_bytes': 36855875,\n",
      " 'httpcompression/response_count': 34,\n",
      " 'item_scraped_count': 825,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 34,\n",
      " 'scheduler/dequeued': 34,\n",
      " 'scheduler/dequeued/memory': 34,\n",
      " 'scheduler/enqueued': 34,\n",
      " 'scheduler/enqueued/memory': 34,\n",
      " 'start_time': datetime.datetime(2022, 9, 29, 14, 24, 44, 316364)}\n",
      "2022-09-29 16:25:03 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Initializing the crawler process\n",
    "filename = \"cities.json\"\n",
    "\n",
    "if filename in os.listdir():\n",
    "        os.remove(filename)\n",
    "\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:92.0) Gecko/20100101 Firefox/92.0',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        filename: {\"format\": \"json\"},\n",
    "    },\n",
    "    \"AUTOTHROTTLE_ENABLED\": True\n",
    "})\n",
    "\n",
    "process.crawl(HotelsbookingSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e786abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2f10f5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(875, 12)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41715792",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./src/filename.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5e9be3726a3e1eb0488b4d4f640ef9d4f9840d98d40cbcadb810c3cb4446c190"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
