{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6083c98",
   "metadata": {},
   "source": [
    "## Conversion Rate Challenge - Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c614b2a8",
   "metadata": {},
   "source": [
    "**Idée Générale** : En regroupant différents modèles ensemble, on obtient une meilleure performance que lorsqu'on utilise nos modèles chacun de leur coté.\n",
    "\n",
    "**Loi des Grands Nombres** : Si on a plus de 50 % de bons résultats, en regroupant nos modèles, nous allons converger vers 100% de bons résultats. Donc, plus le nombre d'individus augmente, plus la performance collective (de la majorité) s'approche des 100% de réussite. Si , au contraire, nous avons moins de 50%, nous allons converger vers 0% de bons résultats."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec91ccb6",
   "metadata": {},
   "source": [
    "        \n",
    "          BAGGING\n",
    "           / \n",
    "          /\n",
    "ENSEMBLE   --- STACKING \n",
    "          \\\n",
    "           \\\n",
    "          BOOSTING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abdf1c",
   "metadata": {},
   "source": [
    "### 1-Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd0acca",
   "metadata": {},
   "source": [
    "**BAGGING**(en Parallele) : \n",
    "\n",
    "On crée plusieurs copies d'un même modèle (comme un Decision Tree), en entrainant chaque copie sur une partie aléatoire du dataset. On utilise une technique d'echantillonnage du dataset appellée **`Bootstrapping`** qui consiste à replacer après chaque tirage les données qui ont été selectionnées dans notre dataset. On obtient ainsi des modèles différents mais entrainés sur des qui peuvent être communes car tirage avec remise. Ca va nous permettre d'obtenir des majorités en faveur des bons résultats.\n",
    "\n",
    "On regroupe enfin les resultats de chaque modèle pour faire des prédictions\n",
    "\n",
    "exemple de Bagging : **Random Forest** qui va utiliser plusieurs **Decision Trees** et qui va utiliser \n",
    "\n",
    "Dans le **Bagging**, les modèles sont tous **`relativement FORT`** (en situation d'**overfitting**), donc le fait de rassembler les prédictions permet de **`réduire la variance`** des différents modèles(cad **réduire l'overfitting**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc9fc8",
   "metadata": {},
   "source": [
    "### 2-Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8306bc1",
   "metadata": {},
   "source": [
    "**BOOSTING**(en Serie):\n",
    "    \n",
    "On entraine l'un après l'autre plusieurs modèles relativement faibles, en demandant à chaque modèle d'essayer de corriger les erreurs effectuées par son prédecesseur. On obtient un ensemble de modèles très complémentaire dans lequel les faiblesses des uns sont compensées par les forces des autres modèles.\n",
    "\n",
    "Il existe deux types d'algorithme de Boosting : **AdaBoost** et **Gradient Boosting**\n",
    "\n",
    "Dans le **Boosting**, les modèles sont tous **`relativement FAIBLE`** (en situation d'**underfitting**) et donc le fait de qu'il s'améliore les uns après les autres permet de **`réduire le biais`** des modèles(car **réduire l'underfitting**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e728237",
   "metadata": {},
   "source": [
    "### 3-Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b95d3",
   "metadata": {},
   "source": [
    "**STACKING**:\n",
    "    \n",
    "Au lieu de **rassembler** les prédictions de chaque modèle pour retenir une **prédiction majoritaire**, on demande à un **`dernier estimateur`** d'apprendre à prédire le resultat final en fonction de ces prédictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d0977",
   "metadata": {},
   "source": [
    "### Résumé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7994b2",
   "metadata": {},
   "source": [
    "**VOTING** : Le modèle le plus simple d'implémentation d'**Ensemble Learning** : on passe tous les modèles sous forme d'une liste, les modèles sont entrainés et les résultats vont être regroupés pour retenir le résultat général.\n",
    "\n",
    "**BAGGING** : Chaque modèle est **FORT** mais **overfit** son subset ==> regrouper les modèles permet de réduire leur variance</br>\n",
    "\n",
    "**BOOSTING** : Chaque modèle est **FAIBLE**, cad en **underfitting** ==> regrouper les modèles permet de réduire leur biais</br>\n",
    "\n",
    "**STACKING** : Un autre estimateur va prédire le résultat final en fonction de ses prédictions\n",
    "\n",
    "**Résultat** : Obtenir un ensemble de modèle qui soit **compétent**, **grand** et **diversifié**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f5fcf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae54189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
